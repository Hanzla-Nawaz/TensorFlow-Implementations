{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t09eeeR5prIJ",
        "papermill": {
          "duration": 0.078286,
          "end_time": "2022-03-16T11:36:01.917192",
          "exception": false,
          "start_time": "2022-03-16T11:36:01.838906",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Text Generation using RNNs\n",
        "\n",
        "RNN Code modified (and simplified) from TensorFlow Website with additional challenges by John Tan Chong Min (2022).\n",
        "\n",
        "Original Notebook at: https://www.tensorflow.org/text/tutorials/text_generation\n",
        "\n",
        "This notebook generates \n",
        "Shakespearean text using a character-based RNN."
      ],
      "id": "t09eeeR5prIJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6",
        "papermill": {
          "duration": 0.074545,
          "end_time": "2022-03-16T11:36:02.829795",
          "exception": false,
          "start_time": "2022-03-16T11:36:02.755250",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Setup"
      ],
      "id": "srXC6pLGLwS6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p",
        "papermill": {
          "duration": 0.071063,
          "end_time": "2022-03-16T11:36:02.973871",
          "exception": false,
          "start_time": "2022-03-16T11:36:02.902808",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ],
      "id": "WGyKZj3bzf9p"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T11:36:03.136322Z",
          "iopub.status.busy": "2022-03-16T11:36:03.135435Z",
          "iopub.status.idle": "2022-03-16T11:36:09.135504Z",
          "shell.execute_reply": "2022-03-16T11:36:09.134702Z"
        },
        "id": "yG_n40gFzf9s",
        "papermill": {
          "duration": 6.086197,
          "end_time": "2022-03-16T11:36:09.135675",
          "exception": false,
          "start_time": "2022-03-16T11:36:03.049478",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "id": "yG_n40gFzf9s"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHDoRoc5PKWz",
        "papermill": {
          "duration": 0.074848,
          "end_time": "2022-03-16T11:36:09.287409",
          "exception": false,
          "start_time": "2022-03-16T11:36:09.212561",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Download the Shakespeare dataset\n",
        "\n",
        "Challenge 1: Change the following line to run this code on your own data."
      ],
      "id": "EHDoRoc5PKWz"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T11:36:09.475790Z",
          "iopub.status.busy": "2022-03-16T11:36:09.473383Z",
          "iopub.status.idle": "2022-03-16T11:36:11.049247Z",
          "shell.execute_reply": "2022-03-16T11:36:11.048437Z"
        },
        "id": "pD_55cOxLkAb",
        "papermill": {
          "duration": 1.678054,
          "end_time": "2022-03-16T11:36:11.049507",
          "exception": false,
          "start_time": "2022-03-16T11:36:09.371453",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac93a8bc-ab49-4ff9-82e9-2ea8c7a013bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "id": "pD_55cOxLkAb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_",
        "papermill": {
          "duration": 0.073722,
          "end_time": "2022-03-16T11:36:11.200708",
          "exception": false,
          "start_time": "2022-03-16T11:36:11.126986",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Read the data\n",
        "\n",
        "First, look in the text:"
      ],
      "id": "UHjdCjDuSvX_"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T11:36:11.358836Z",
          "iopub.status.busy": "2022-03-16T11:36:11.357816Z",
          "iopub.status.idle": "2022-03-16T11:36:11.363527Z",
          "shell.execute_reply": "2022-03-16T11:36:11.364116Z"
        },
        "id": "aavnuByVymwK",
        "outputId": "3c85d8f8-0128-4a37-d0f5-e3f9c5f31234",
        "papermill": {
          "duration": 0.086137,
          "end_time": "2022-03-16T11:36:11.364293",
          "exception": false,
          "start_time": "2022-03-16T11:36:11.278156",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "id": "aavnuByVymwK"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T11:36:11.714368Z",
          "iopub.status.busy": "2022-03-16T11:36:11.713560Z",
          "iopub.status.idle": "2022-03-16T11:36:11.717309Z",
          "shell.execute_reply": "2022-03-16T11:36:11.716642Z"
        },
        "id": "Duhg9NrUymwO",
        "outputId": "643a0ca4-ed0f-4d37-8034-8f4e11b34592",
        "papermill": {
          "duration": 0.092887,
          "end_time": "2022-03-16T11:36:11.717457",
          "exception": false,
          "start_time": "2022-03-16T11:36:11.624570",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "id": "Duhg9NrUymwO"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T11:36:11.888085Z",
          "iopub.status.busy": "2022-03-16T11:36:11.887045Z",
          "iopub.status.idle": "2022-03-16T11:36:11.891239Z",
          "shell.execute_reply": "2022-03-16T11:36:11.891743Z"
        },
        "id": "IlCgQBRVymwR",
        "outputId": "cef240ad-2b43-45c0-d40e-6eda666fa320",
        "papermill": {
          "duration": 0.097218,
          "end_time": "2022-03-16T11:36:11.891931",
          "exception": false,
          "start_time": "2022-03-16T11:36:11.794713",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A total of 65 unique characters:\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'A total of {len(vocab)} unique characters:')\n",
        "print(sorted(list(set(text))))"
      ],
      "id": "IlCgQBRVymwR"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hV474cnaUj-D"
      },
      "outputs": [],
      "source": [
        "char_to_num = {char: num for num, char in enumerate(vocab)}\n",
        "num_to_char = {num: char for num, char in enumerate(vocab)}"
      ],
      "id": "hV474cnaUj-D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCVhz6GDUj-E"
      },
      "source": [
        "# Split the input text and index the text\n",
        "\n",
        "Pass the indexed text into a TensorFlow dataset, then make the dataset be in batches of *seq_length* + 1.\n",
        "\n",
        "Within each batch, we then make the input text the first *seq_length* characters, and the output text is last *seq_length* characters (input right shifted by 1)."
      ],
      "id": "DCVhz6GDUj-E"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "71ybMFLkUj-E"
      },
      "outputs": [],
      "source": [
        "def get_text(index_list, default = ' '):\n",
        "    ''' This gets back the text based on a list of indices \n",
        "    If index is not present in dictionary, defaults to ' ' '''\n",
        "    return ''.join(num_to_char[k.numpy()] if k.numpy() in num_to_char else default for k in index_list)\n",
        "\n",
        "def get_indices(char_list, default = 0):\n",
        "    ''' This gets the indices based on the list of text \n",
        "    If character is not present in dictionary, defaults to index 0\n",
        "    '''\n",
        "    return [char_to_num[c] if c in char_to_num else default for c in char_list]"
      ],
      "id": "71ybMFLkUj-E"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jF5IAQ34Uj-E"
      },
      "outputs": [],
      "source": [
        "all_ids = get_indices(text)\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "id": "jF5IAQ34Uj-E"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EED7GV1lUj-F",
        "outputId": "de719e75-a038-42d8-ae1c-7ed196acf1f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(18, shape=(), dtype=int32) F\n",
            "tf.Tensor(47, shape=(), dtype=int32) i\n",
            "tf.Tensor(56, shape=(), dtype=int32) r\n",
            "tf.Tensor(57, shape=(), dtype=int32) s\n",
            "tf.Tensor(58, shape=(), dtype=int32) t\n",
            "tf.Tensor(1, shape=(), dtype=int32)  \n",
            "tf.Tensor(15, shape=(), dtype=int32) C\n",
            "tf.Tensor(47, shape=(), dtype=int32) i\n",
            "tf.Tensor(58, shape=(), dtype=int32) t\n",
            "tf.Tensor(47, shape=(), dtype=int32) i\n"
          ]
        }
      ],
      "source": [
        "# See the first 10 characters of the text\n",
        "for ids in ids_dataset.take(10):\n",
        "    print(ids, get_text([ids]))"
      ],
      "id": "EED7GV1lUj-F"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhWewH7zUj-F",
        "outputId": "16494ba2-b14a-40ff-d810-7d0bee9805e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexed Sequence:\n",
            "tf.Tensor(\n",
            "[18 47 56 57 58  1 15 47 58 47 64 43 52 10  0 14 43 44 53 56 43  1 61 43\n",
            "  1 54 56 53 41 43 43 42  1 39 52 63  1 44 59 56 58 46 43 56  6  1 46 43\n",
            " 39 56  1 51 43  1 57 54 43 39 49  8  0  0 13 50 50 10  0 31 54 43 39 49\n",
            "  6  1 57 54 43 39 49  8  0  0 18 47 56 57 58  1 15 47 58 47 64 43 52 10\n",
            "  0 37 53 59  1], shape=(101,), dtype=int32)\n",
            "Actual Sequence:\n",
            "['First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou ']\n"
          ]
        }
      ],
      "source": [
        "seq_length = 100\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "for seq in sequences.take(1):\n",
        "    print('Indexed Sequence:')\n",
        "    print(seq)\n",
        "    print('Actual Sequence:')\n",
        "    print([get_text(seq)])"
      ],
      "id": "uhWewH7zUj-F"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YrdwzEUiUj-F"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = tf.squeeze(sequence[:-1])\n",
        "    target_text = tf.squeeze(sequence[1:])\n",
        "    return input_text, target_text"
      ],
      "id": "YrdwzEUiUj-F"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "r2NRsApUUj-G"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "id": "r2NRsApUUj-G"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMGRgmznUj-G",
        "outputId": "4e212d6b-462e-4687-d360-8bfad9557b95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (100,)\n",
            "Output shape: (100,)\n",
            "Input : ['First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou']\n",
            "Target: ['irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou ']\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input shape:\", input_example.shape)\n",
        "    print(\"Output shape:\", target_example.shape)\n",
        "    print(\"Input :\", [get_text(input_example)])\n",
        "    print(\"Target:\", [get_text(target_example)])"
      ],
      "id": "KMGRgmznUj-G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7Gm6zwHUj-G"
      },
      "source": [
        "# Creating suitably-sized datasets\n",
        "\n",
        "Add in a buffer so that we do not load the entire dataset at one go (this dataset is huge).\n",
        "We also add in batching for faster training."
      ],
      "id": "p7Gm6zwHUj-G"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "G9yr7jNcUj-G"
      },
      "outputs": [],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "id": "G9yr7jNcUj-G"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx",
        "papermill": {
          "duration": 0.086166,
          "end_time": "2022-03-16T11:36:18.737364",
          "exception": false,
          "start_time": "2022-03-16T11:36:18.651198",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Build The Model"
      ],
      "id": "r6oUuElIMgVx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkA5upJIJ7W7",
        "papermill": {
          "duration": 0.085161,
          "end_time": "2022-03-16T11:36:19.645295",
          "exception": false,
          "start_time": "2022-03-16T11:36:19.560134",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "For each character the model looks up the embedding, we run the GRU one timestep with the embedding as input, and apply the dense layer to generate logits predicting the log-likelihood of the next character:\n",
        "\n",
        "![A drawing of the data passing through the model](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_training.png?raw=1)"
      ],
      "id": "RkA5upJIJ7W7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8gPwEjRzf-Z",
        "papermill": {
          "duration": 0.082566,
          "end_time": "2022-03-16T11:36:18.905109",
          "exception": false,
          "start_time": "2022-03-16T11:36:18.822543",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "This model has three layers:\n",
        "\n",
        "* `tf.keras.layers.Embedding`: The input layer. A trainable lookup table that will map each character-ID to a vector with `embedding_dim` dimensions;\n",
        "* `tf.keras.layers.GRU`: A type of RNN with size `units=rnn_units` (You can also use an LSTM layer here.)\n",
        "* `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs. It outputs one logit for each character in the vocabulary. These are the log-likelihood of each character according to the model.\n",
        "\n",
        "The model takes in an entire *seq_length* of text as an input (can be any length), and outputs logit probabilities of the next character for each character of the input."
      ],
      "id": "m8gPwEjRzf-Z"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T11:36:19.081151Z",
          "iopub.status.busy": "2022-03-16T11:36:19.080339Z",
          "iopub.status.idle": "2022-03-16T11:36:19.082537Z",
          "shell.execute_reply": "2022-03-16T11:36:19.083093Z"
        },
        "id": "zHT8cLh7EAsg",
        "papermill": {
          "duration": 0.093165,
          "end_time": "2022-03-16T11:36:19.083289",
          "exception": false,
          "start_time": "2022-03-16T11:36:18.990124",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN hidden nodes\n",
        "rnn_units = 1024"
      ],
      "id": "zHT8cLh7EAsg"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FFbgTbRwUj-H"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(batch_input_shape = (None, None)),\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim), \n",
        "    # Let us create a GRU layer which returns all intermediate sequences\n",
        "    # Challenge 2: For RNN use tf.keras.layers.SimpleRNN, For LSTM use tf.keras.layers.LSTM\n",
        "    tf.keras.layers.GRU(rnn_units, return_sequences = True), # Note you can also try out RNN or LSTM\n",
        "    tf.keras.layers.Dense(vocab_size, activation = tf.nn.softmax)]\n",
        ")"
      ],
      "id": "FFbgTbRwUj-H"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also run a similar code using the Functional API. Check it out by uncommenting the cell below!"
      ],
      "metadata": {
        "id": "bR0SP8wwfuF3"
      },
      "id": "bR0SP8wwfuF3"
    },
    {
      "cell_type": "code",
      "source": [
        "# inputs = tf.keras.layers.Input(batch_input_shape = (None, None))\n",
        "# x = tf.keras.layers.Embedding(vocab_size, embedding_dim)(inputs)\n",
        "# x = tf.keras.layers.GRU(rnn_units, return_sequences = True)(x)\n",
        "# outputs = tf.keras.layers.Dense(vocab_size, activation = tf.nn.softmax)(x)\n",
        "# model = tf.keras.Model(inputs = inputs, outputs = outputs)"
      ],
      "metadata": {
        "id": "jauMLRynfeZk"
      },
      "id": "jauMLRynfeZk",
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NzLBi4VM4o",
        "papermill": {
          "duration": 0.092412,
          "end_time": "2022-03-16T11:36:25.470717",
          "exception": false,
          "start_time": "2022-03-16T11:36:25.378305",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "In the above example the sequence length of the input is `100` but the model can be run on inputs of any length:"
      ],
      "id": "Q6NzLBi4VM4o"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T11:36:25.663897Z",
          "iopub.status.busy": "2022-03-16T11:36:25.662465Z",
          "iopub.status.idle": "2022-03-16T11:36:25.668781Z",
          "shell.execute_reply": "2022-03-16T11:36:25.667803Z"
        },
        "id": "vPGmAAXmVLGC",
        "outputId": "2a0efeef-773d-44fb-9ec7-735a6377d782",
        "papermill": {
          "duration": 0.107527,
          "end_time": "2022-03-16T11:36:25.668997",
          "exception": false,
          "start_time": "2022-03-16T11:36:25.561470",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 256)         16640     \n",
            "                                                                 \n",
            " gru (GRU)                   (None, None, 1024)        3938304   \n",
            "                                                                 \n",
            " dense (Dense)               (None, None, 65)          66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,021,569\n",
            "Trainable params: 4,021,569\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ],
      "id": "vPGmAAXmVLGC"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "OM7jEonrUj-I",
        "outputId": "e3236d4f-df4c-45d7-c27d-38e1cf2f2190"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Image object>"
            ],
            "image/png": "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\n"
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "tf.keras.utils.plot_model(model)"
      ],
      "id": "OM7jEonrUj-I"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T11:36:28.904265Z",
          "iopub.status.busy": "2022-03-16T11:36:28.903364Z",
          "iopub.status.idle": "2022-03-16T11:36:28.917000Z",
          "shell.execute_reply": "2022-03-16T11:36:28.917516Z"
        },
        "id": "DDl1_Een6rL0",
        "papermill": {
          "duration": 0.113405,
          "end_time": "2022-03-16T11:36:28.917744",
          "exception": false,
          "start_time": "2022-03-16T11:36:28.804339",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer = tf.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "id": "DDl1_Een6rL0"
    },
    {
      "cell_type": "code",
      "source": [
        "for i, o in dataset.take(1):\n",
        "  print('Input (with batch dimension) shape:', i.shape, 'Output (with batch dimension) shape:', o.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx0R1Ficf_Lh",
        "outputId": "c807137e-546f-4e4f-8b72-8846f4ab7ae4"
      },
      "id": "Yx0R1Ficf_Lh",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input (with batch dimension) shape: (64, 100) Output (with batch dimension) shape: (64, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7md6UenCUj-I",
        "outputId": "1516be09-b2f6-4d2d-e6c8-25157fa73b99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "172/172 [==============================] - 32s 140ms/step - loss: 2.7301 - accuracy: 0.2781\n",
            "Epoch 2/5\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 1.9905 - accuracy: 0.4181\n",
            "Epoch 3/5\n",
            "172/172 [==============================] - 25s 138ms/step - loss: 1.7145 - accuracy: 0.4915\n",
            "Epoch 4/5\n",
            "172/172 [==============================] - 25s 137ms/step - loss: 1.5539 - accuracy: 0.5338\n",
            "Epoch 5/5\n",
            "172/172 [==============================] - 25s 137ms/step - loss: 1.4550 - accuracy: 0.5595\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4530204310>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "model.fit(dataset, epochs=5)"
      ],
      "id": "7md6UenCUj-I"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwv0gEkURfx1",
        "papermill": {
          "duration": 0.092919,
          "end_time": "2022-03-16T11:36:25.852269",
          "exception": false,
          "start_time": "2022-03-16T11:36:25.759350",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop."
      ],
      "id": "uwv0gEkURfx1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN",
        "papermill": {
          "duration": 0.761835,
          "end_time": "2022-03-16T12:47:55.186076",
          "exception": false,
          "start_time": "2022-03-16T12:47:54.424241",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Generate text"
      ],
      "id": "kKkD5M6eoSiN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIdQ8c8NvMzV",
        "papermill": {
          "duration": 0.767086,
          "end_time": "2022-03-16T12:47:56.740452",
          "exception": false,
          "start_time": "2022-03-16T12:47:55.973366",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "We will feed in an input of one character, and ask the model to predict the next character.\n",
        "\n",
        "The next character is sampled based on the logits output at the current time step, and fed to the model in the next time step.\n",
        "\n",
        "\n",
        "For the next character, we will feed in the entire set of characters from start time till that character, and ask the model to do the prediction. (NOTE: We could also do it by passing the model's internal state each time step to the next time step, and then just pass through one character, but doing so will require to do more complicated modelling.)\n",
        "\n",
        "![To generate text the model's output is fed back to the input](https://github.com/tensorflow/text/blob/master/docs/tutorials/images/text_generation_sampling.png?raw=1)"
      ],
      "id": "oIdQ8c8NvMzV"
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_next_char(input_text = \"ROMEO:\", temp = 0.02):\n",
        "\n",
        "  ''' Given the input text, generate the next character\n",
        "  Inputs: \n",
        "  input_text (str): The text from start till now\n",
        "  temp (float): Controls how random the sampling is (< 1: similar to argmax, > 1: more random, = 1: original softmax probabilities)\n",
        "  '''\n",
        "\n",
        "  predicted_logits = model(tf.reshape(tf.convert_to_tensor(get_indices(input_text)), (1, -1)))\n",
        "\n",
        "  # Only use the last logit prediction\n",
        "  predicted_logits = predicted_logits[:, -1, :]\n",
        "  predicted_logits = predicted_logits/temp\n",
        "\n",
        "  # Sample the output logits to generate token IDs.\n",
        "  predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)[0]\n",
        "  return get_text(predicted_ids)"
      ],
      "metadata": {
        "id": "vFu2kscoW9RI"
      },
      "id": "vFu2kscoW9RI",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_next_char(\"ROMEO: \"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ekfd_sCVqdHl",
        "outputId": "47370f1e-5e71-4d0b-f7e1-ed9aa50cee0d"
      },
      "id": "Ekfd_sCVqdHl",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9yDoa0G3IgQ",
        "papermill": {
          "duration": 0.800943,
          "end_time": "2022-03-16T12:48:03.067164",
          "exception": false,
          "start_time": "2022-03-16T12:48:02.266221",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ],
      "id": "p9yDoa0G3IgQ"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-03-16T12:48:04.638404Z",
          "iopub.status.busy": "2022-03-16T12:48:04.631908Z",
          "iopub.status.idle": "2022-03-16T12:48:08.726797Z",
          "shell.execute_reply": "2022-03-16T12:48:08.727643Z"
        },
        "id": "ST7PSyk9t1mT",
        "outputId": "bf13682b-1a5f-4328-ead2-458689476204",
        "papermill": {
          "duration": 4.877571,
          "end_time": "2022-03-16T12:48:08.727904",
          "exception": false,
          "start_time": "2022-03-16T12:48:03.850333",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "The first thou hast not so the world in the brother\n",
            "That the matter that the prince of his country,\n",
            "That have been so fair and all the seatest of the wind\n",
            "That have been so fair and unto the prince.\n",
            "\n",
            "KING RICHARD III:\n",
            "What should be so for the sentence of the seaters of the Bolingbroke.\n",
            "\n",
            "KING RICHARD III:\n",
            "The sent the prince of his country and the prince of his country\n",
            "That which the best of the \n",
            "\n",
            "Run time: 6.238189697265625\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "num_char = 400      # Number of characters to generate\n",
        "num_prev_char = 100 # Number of previous characters to focus on\n",
        "mytext = \"ROMEO:\"\n",
        "\n",
        "for n in range(num_char):\n",
        "  next_char = generate_next_char(mytext[-num_prev_char:])\n",
        "  mytext += next_char\n",
        "\n",
        "print(mytext)\n",
        "end = time.time()\n",
        "print('\\nRun time:', end - start)"
      ],
      "id": "ST7PSyk9t1mT"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Challenges\n",
        "\n",
        "Have fun playing around with this code and learn about RNNs!\n",
        "\n",
        "- Challenge 1: Use your own dataset and run the code to see if it can fit to your dataset\n",
        "- Challenge 2: Change the code to LSTM / RNN and test out which is better\n",
        "- Challenge 3: See if you can improve the output plausibility by modifying the embedding_dim, rnn_units, or epochs. (Hint: epochs is the most straightforward)\n",
        "\n",
        "\n",
        "For Challenge 2, for the same number of training epochs, you will find that LSTM/GRU model outputs are significantly better than those of RNN. This may be due to the \"vanishing/exploding\" gradient problem in the vanilla RNN."
      ],
      "metadata": {
        "id": "At_WyIZ4rZHc"
      },
      "id": "At_WyIZ4rZHc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Challenge\n",
        "Can the model still work if we replace the Embedding Layer with One Hot Encoding instead?"
      ],
      "metadata": {
        "id": "qUzJGTbf5PRj"
      },
      "id": "qUzJGTbf5PRj"
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessing_layer = tf.keras.layers.CategoryEncoding(vocab_size, 'one_hot')\n",
        "\n",
        "dataset2 = sequences.map(split_input_target)\n",
        "dataset2 = (\n",
        "    dataset2\n",
        "    .map(lambda x, y: (preprocessing_layer(x), y))\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))"
      ],
      "metadata": {
        "id": "Mlwvz-exvTXg"
      },
      "id": "Mlwvz-exvTXg",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, o in dataset2.take(1):\n",
        "  print(i.shape, o.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znmnY-Vk6Yc_",
        "outputId": "cb4fb966-ebff-40a5-e510-3ee429a19386"
      },
      "id": "znmnY-Vk6Yc_",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) (64, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Redo the model with the new one-hot-encoding prepocessing and remove Embedding layer."
      ],
      "metadata": {
        "id": "E5xUGWaD6q8H"
      },
      "id": "E5xUGWaD6q8H"
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(batch_input_shape = (None, None, vocab_size)),\n",
        "    tf.keras.layers.GRU(rnn_units, return_sequences = True), \n",
        "    tf.keras.layers.Dense(vocab_size, activation = tf.nn.softmax)]\n",
        ")"
      ],
      "metadata": {
        "id": "rfcbtHi46jaT"
      },
      "id": "rfcbtHi46jaT",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1ToOcxG6xHy",
        "outputId": "34311ab2-cc46-40cc-f68d-68ee6898a5d3"
      },
      "id": "g1ToOcxG6xHy",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_1 (GRU)                 (None, None, 1024)        3351552   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, None, 65)          66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,418,177\n",
            "Trainable params: 3,418,177\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(optimizer = tf.optimizers.Adam(),\n",
        "              loss = 'sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "FsCdakgC6yMD"
      },
      "id": "FsCdakgC6yMD",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.fit(dataset2, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chGFkTsd62C0",
        "outputId": "1eab3ff1-6b61-4eb3-de50-99c00fc063a9"
      },
      "id": "chGFkTsd62C0",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "172/172 [==============================] - 25s 125ms/step - loss: 2.8971 - accuracy: 0.2465\n",
            "Epoch 2/5\n",
            "172/172 [==============================] - 24s 126ms/step - loss: 2.1988 - accuracy: 0.3705\n",
            "Epoch 3/5\n",
            "172/172 [==============================] - 24s 125ms/step - loss: 2.0188 - accuracy: 0.4154\n",
            "Epoch 4/5\n",
            "172/172 [==============================] - 24s 125ms/step - loss: 1.8677 - accuracy: 0.4520\n",
            "Epoch 5/5\n",
            "172/172 [==============================] - 24s 125ms/step - loss: 1.7437 - accuracy: 0.4835\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f45b2ee4810>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_next_char_onehot(input_text = \"ROMEO:\", temp = 0.02):\n",
        "\n",
        "  ''' Given the input text, generate the next character using one-hot encoding\n",
        "  Inputs: \n",
        "  input_text (str): The text from start till now\n",
        "  temp (float): Controls how random the sampling is (< 1: similar to argmax, > 1: more random, = 1: original softmax probabilities)\n",
        "  '''\n",
        "\n",
        "  indices = tf.reshape(tf.convert_to_tensor(get_indices(input_text)), (1, -1))\n",
        "  predicted_logits = model2(tf.one_hot(indices, vocab_size))\n",
        "\n",
        "  # Only use the last logit prediction\n",
        "  predicted_logits = predicted_logits[:, -1, :]\n",
        "  predicted_logits = predicted_logits/temp\n",
        "\n",
        "  # Sample the output logits to generate token IDs.\n",
        "  predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)[0]\n",
        "  return get_text(predicted_ids)"
      ],
      "metadata": {
        "id": "NjT5kWI_73Vt"
      },
      "id": "NjT5kWI_73Vt",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "num_char = 400      # Number of characters to generate\n",
        "num_prev_char = 100 # Number of previous characters to focus on\n",
        "mytext = \"ROMEO:\"\n",
        "\n",
        "for n in range(num_char):\n",
        "  next_char = generate_next_char_onehot(mytext[-num_prev_char:])\n",
        "  mytext += next_char\n",
        "\n",
        "print(mytext)\n",
        "end = time.time()\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rT-dKXN64Ul",
        "outputId": "0a38487f-e798-499e-be48-cbc3f32e4048"
      },
      "id": "-rT-dKXN64Ul",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "I have be the stand of the come,\n",
            "And be the will be sone to see the rester with a mand\n",
            "That shall be sone and , and be the beath,\n",
            "And the beath me the rest good man.\n",
            "\n",
            "Second Catizen:\n",
            "What is the read of the Ore the will be the death,\n",
            "And we propest the death of the -and and the seath,\n",
            "And shall be the will so mane and be the will.\n",
            "\n",
            "KING HENRY VI:\n",
            "What is the rest and be the couster of the reads,\n",
            "\n",
            "\n",
            "Run time: 6.005336761474609\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the generated text is not as good as that with Embeddings. Embedding Layers help to put the vocabulary into a relevant semantic space to help it to generate better text.\n",
        "\n",
        "Even in a character-level model, similar character (capital or non-capital) may be semantically related, so they can be grouped in the same semantic space for better generalization."
      ],
      "metadata": {
        "id": "s_wDHBNS_UIi"
      },
      "id": "s_wDHBNS_UIi"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XrQ-QaGj_nL_"
      },
      "id": "XrQ-QaGj_nL_",
      "execution_count": 33,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 4371.829277,
      "end_time": "2022-03-16T12:48:40.972849",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-03-16T11:35:49.143572",
      "version": "2.3.3"
    },
    "colab": {
      "name": "RNN_Text_Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}