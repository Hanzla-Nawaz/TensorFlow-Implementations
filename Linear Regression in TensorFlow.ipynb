{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the y = mx + b problem\n",
    "\n",
    "Given 5 training data of (x, y) tuples: (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), \n",
    "solve for **m** and **b**.\n",
    "\n",
    "- Forward pass prediction: $\\hat{y} = mx + b$\n",
    "- Loss: $0.5*(y - \\hat{y})^2$\n",
    "\n",
    "We attempt to solve the credit assignment problem:\n",
    "- We increase the value of m and b and observe whether the loss increases or decreases.\n",
    "- If the loss increases, then we decrement **m** or **b**\n",
    "- Otherwise, we increment **m** or **b**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 24.304392000000004, w: 0.005, b: 0.005\n",
      "\n",
      "Epoch 100: Loss: 6.027391999999992, w: 0.5050000000000003, b: 0.5050000000000003\n",
      "\n",
      "Epoch 200: Loss: 9.860761315262648e-30, w: 0.9990000000000007, b: 0.9990000000000007\n",
      "\n",
      "Epoch 300: Loss: 9.860761315262648e-30, w: 0.9990000000000007, b: 0.9990000000000007\n",
      "\n",
      "Epoch 400: Loss: 9.860761315262648e-30, w: 0.9990000000000007, b: 0.9990000000000007\n",
      "\n",
      "Epoch 500: Loss: 9.860761315262648e-30, w: 0.9990000000000007, b: 0.9990000000000007\n",
      "\n",
      "Epoch 600: Loss: 9.860761315262648e-30, w: 0.9990000000000007, b: 0.9990000000000007\n",
      "\n",
      "Epoch 700: Loss: 9.860761315262648e-30, w: 0.9990000000000007, b: 0.9990000000000007\n",
      "\n",
      "Epoch 800: Loss: 9.860761315262648e-30, w: 0.9990000000000007, b: 0.9990000000000007\n",
      "\n",
      "Epoch 900: Loss: 9.860761315262648e-30, w: 0.9990000000000007, b: 0.9990000000000007\n",
      "\n",
      "Equation is y = 1.0x + 1.0\n"
     ]
    }
   ],
   "source": [
    "x_values = [2, 3, 4, 5, 6]\n",
    "y_values = [3, 4, 5, 6, 7]\n",
    "\n",
    "# initialize w and b to be 0\n",
    "w, b = 0, 0\n",
    "\n",
    "# Customize your parameters here\n",
    "epochs = 1000\n",
    "lr = 0.001\n",
    "perturb = 0.001\n",
    "\n",
    "# Perform the training loop\n",
    "for epoch in range(epochs):\n",
    "    for (x,y) in zip(x_values, y_values):\n",
    "        y_pred = w*x + b\n",
    "        loss = 0.5*(y - y_pred)**2\n",
    "        \n",
    "        # do perturbation to find out how much to change w and b\n",
    "        w_perturb = w + perturb\n",
    "        y_pred_perturb_w = w_perturb*x + b\n",
    "        loss_perturb_w = 0.5*(y_pred_perturb_w - y)**2\n",
    "        \n",
    "        b_perturb = b + perturb\n",
    "        y_pred_perturb_b = w*x + b_perturb\n",
    "        loss_perturb_b = 0.5*(y_pred_perturb_b - y)**2\n",
    "        \n",
    "        # modify w and b accordingly\n",
    "        if loss_perturb_w < loss:\n",
    "            w += lr\n",
    "        else:\n",
    "            w -= lr\n",
    "        \n",
    "        if loss_perturb_b < loss:\n",
    "            b += lr\n",
    "        else:\n",
    "            b -= lr\n",
    "            \n",
    "    if epoch%100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss: {loss}, w: {w}, b: {b}\\n\")\n",
    "        \n",
    "print(f\"Equation is y = {w:.1f}x + {b:.1f}\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to do it with backpropagation\n",
    "\n",
    "Instead of using just a fixed learning rate value, we multiply it by the gradient with respect to the parameter.\n",
    "\n",
    "In other words, we are implementing: \n",
    "- $w = w-lr*\\frac{\\partial L}{\\partial w}$\n",
    "- $b = b-lr*\\frac{\\partial L}{\\partial b}$\n",
    "\n",
    "Here, \n",
    "\n",
    "- $L = 0.5*(y - \\hat{y})^2$\n",
    "\n",
    "- $\\hat{y} = wx + b$\n",
    "\n",
    "- $\\frac{\\partial L}{\\partial \\hat{y}} = (y - \\hat{y})(-1) = \\hat{y} - y$\n",
    "\n",
    "- $\\frac{\\partial \\hat{y}}{\\partial w} = x$\n",
    "\n",
    "- $\\frac{\\partial \\hat{y}}{\\partial b} = 1$\n",
    "\n",
    "Hence, $\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial w} = (\\hat{y})(-1)x$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial \\hat{y}}\\frac{\\partial \\hat{y}}{\\partial b} = (y - \\hat{y})(1) = (y - \\hat{y})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 21.665097496865293, w: 0.10611744338133801, b: 0.024282829774723\n",
      "\n",
      "Epoch 100: Loss: 0.02834757250261382, w: 1.1548209956639715, b: 0.3003714746028339\n",
      "\n",
      "Epoch 200: Loss: 0.025584115471937192, w: 1.1468576141058933, b: 0.336688727618972\n",
      "\n",
      "Epoch 300: Loss: 0.022998040375833806, w: 1.139237633242645, b: 0.371105891600474\n",
      "\n",
      "Epoch 400: Loss: 0.020673362999295952, w: 1.1320130246723028, b: 0.4037372546866741\n",
      "\n",
      "Epoch 500: Loss: 0.018583667595460446, w: 1.125163278613873, b: 0.434675477635787\n",
      "\n",
      "Epoch 600: Loss: 0.01670520182470304, w: 1.1186689446155902, b: 0.4640084122337957\n",
      "\n",
      "Epoch 700: Loss: 0.015016614270060393, w: 1.1125115814489146, b: 0.49181935191014\n",
      "\n",
      "Epoch 800: Loss: 0.013498711748714864, w: 1.1066737047434116, b: 0.5181872682567613\n",
      "\n",
      "Epoch 900: Loss: 0.012134241154359411, w: 1.1011387373383543, b: 0.5431870352748401\n",
      "\n",
      "Equation is y = 1.1x + 0.6\n"
     ]
    }
   ],
   "source": [
    "x_values = [2, 3, 4, 5, 6]\n",
    "y_values = [3, 4, 5, 6, 7]\n",
    "\n",
    "# initialize w and b to be 0\n",
    "w, b = 0, 0\n",
    "\n",
    "# Customize your parameters here\n",
    "epochs = 1000\n",
    "lr = 0.001\n",
    "\n",
    "# Perform the training loop\n",
    "for epoch in range(epochs):\n",
    "    for (x,y) in zip(x_values, y_values):\n",
    "        y_pred = w*x + b\n",
    "#         loss = 0.5*(y - y_pred)**2\n",
    "        \n",
    "        # use gradient descent to update w and b\n",
    "        w += lr * (y-y_pred)*(x)\n",
    "        b += lr * (y-y_pred)\n",
    "            \n",
    "    if epoch%100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss: {loss}, w: {w}, b: {b}\\n\")\n",
    "        \n",
    "print(f\"Equation is y = {w:.1f}x + {b:.1f}\")      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow for Linear Regression\n",
    "\n",
    "Now let us use TensorFlow for Linear Regression.\n",
    "\n",
    "This is actually an advanced version of TensorFlow, as it uses GradientTape.\n",
    "\n",
    "We do this so that you can see the underlying features of what TensorFlow is doing under the hood.\n",
    "\n",
    "Next week, we will see that TensorFlow has many abstractions that can allow an easy training pipeline (without needing to code at this level, unless you want to become a researcher next time XD).\n",
    "\n",
    "### You try it: Try changing the learning rate / epoch and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss: 6.254148960113525, w: 0.7640827894210815, b: 0.1873777210712433\n",
      "\n",
      "Epoch 100: Loss: 0.009376347064971924, w: 1.082818865776062, b: 0.5893599390983582\n",
      "\n",
      "Epoch 200: Loss: 0.0030395882204174995, w: 1.0471539497375488, b: 0.7661970257759094\n",
      "\n",
      "Epoch 300: Loss: 0.0009853508090600371, w: 1.0268474817276, b: 0.8668816685676575\n",
      "\n",
      "Epoch 400: Loss: 0.0003194306918885559, w: 1.0152859687805176, b: 0.9242074489593506\n",
      "\n",
      "Epoch 500: Loss: 0.00010354965343140066, w: 1.008703351020813, b: 0.9568465948104858\n",
      "\n",
      "Epoch 600: Loss: 3.3570569939911366e-05, w: 1.0049554109573364, b: 0.9754296541213989\n",
      "\n",
      "Epoch 700: Loss: 1.0880636182264425e-05, w: 1.002821445465088, b: 0.9860103130340576\n",
      "\n",
      "Epoch 800: Loss: 3.528389243001584e-06, w: 1.0016064643859863, b: 0.9920346140861511\n",
      "\n",
      "Epoch 900: Loss: 1.14314855181874e-06, w: 1.0009146928787231, b: 0.9954645037651062\n",
      "\n",
      "Equation is y = 1.0x + 1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_values = [2, 3, 4, 5, 6]\n",
    "y_values = [3, 4, 5, 6, 7]\n",
    "\n",
    "# Initialize w and b to be 0\n",
    "w = tf.Variable([0.0])\n",
    "b = tf.Variable([0.0])\n",
    "\n",
    "# Customize your parameters here\n",
    "epochs = 1000\n",
    "lr = 0.01\n",
    "\n",
    "# Perform the training loop\n",
    "for epoch in range(epochs):\n",
    "    for (x,y) in zip(x_values, y_values):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # do your model forward pass here\n",
    "            y_pred = w*x + b\n",
    "\n",
    "            # do loss function here\n",
    "            loss = 0.5*(y - y_pred)**2\n",
    "\n",
    "        grads = tape.gradient(loss, [w,b])\n",
    "       \n",
    "        w.assign_sub(lr * grads[0])\n",
    "        b.assign_sub(lr * grads[1])\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss: {loss[0]}, w: {w.numpy()[0]}, b: {b.numpy()[0]}\\n\")\n",
    "        \n",
    "print(f\"Equation is y = {w.numpy()[0]:.1f}x + {b.numpy()[0]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Course 1 - Part 4 - Lesson 2 - Notebook.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
